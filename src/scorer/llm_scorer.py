"""å…¨LLMç»Ÿä¸€è¯„åˆ†å¼•æ“ - Phase 9é‡æ„ç‰ˆ

å®Œå…¨åŸºäºLLMçš„å•æ¬¡è°ƒç”¨ç»Ÿä¸€è¯„åˆ†æ¶æ„ï¼Œå–ä»£ç¡¬ç¼–ç è§„åˆ™å’Œå¤šæ¬¡è°ƒç”¨ï¼Œè¿½æ±‚æœ€é«˜æ•°æ®è´¨é‡å’Œå¯è§£é‡Šæ€§ã€‚

å…³é”®ç‰¹æ€§ï¼š
- å•æ¬¡LLMè°ƒç”¨è¿”å›å…¨éƒ¨26ä¸ªå­—æ®µ
- 4000+ tokenè¶…è¯¦ç»†promptï¼ŒåŒ…å«MGXåœºæ™¯å®šä¹‰å’Œè¯„åˆ†æ ‡å‡†
- å¼ºåˆ¶å­—æ®µå®Œæˆï¼Œä¸å…è®¸N/Aè¿”å›
- æ¯ä¸ªç»´åº¦150+å­—è¯¦ç»†æ¨ç†ï¼Œåç«¯ä¸“é¡¹200+å­—è¯¦ç»†æ¨ç†
- æ€»æ¨ç†å­—æ•°â‰¥1200å­—ç¬¦ï¼Œç¡®ä¿å®Œæ•´å¯è§£é‡Šæ€§
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
from typing import Any, Awaitable, Dict, List, Optional, Tuple, cast

import redis.asyncio as redis
from redis.asyncio import Redis as AsyncRedis
from openai import AsyncOpenAI
from openai.types.chat import ChatCompletionMessageParam
from pydantic import BaseModel, ConfigDict, Field, ValidationError, field_validator
from tenacity import retry, stop_after_attempt, wait_exponential

from src.common import clean_summary_text, constants
from src.config import get_settings
from src.models import RawCandidate, ScoredCandidate

logger = logging.getLogger(__name__)

REASONING_FIELD_ORDER = [
    "activity_reasoning",
    "reproducibility_reasoning",
    "license_reasoning",
    "novelty_reasoning",
    "relevance_reasoning",
    "backend_mgx_reasoning",
    "backend_engineering_reasoning",
    "overall_reasoning",
]

REASONING_FIELD_LABELS = {
    "activity_reasoning": "activity_reasoningï¼ˆæ´»è·ƒåº¦æ¨ç†ï¼‰",
    "reproducibility_reasoning": "reproducibility_reasoningï¼ˆå¯å¤ç°æ€§æ¨ç†ï¼‰",
    "license_reasoning": "license_reasoningï¼ˆè®¸å¯æ¨ç†ï¼‰",
    "novelty_reasoning": "novelty_reasoningï¼ˆæ–°é¢–æ€§æ¨ç†ï¼‰",
    "relevance_reasoning": "relevance_reasoningï¼ˆMGXç›¸å…³æ€§æ¨ç†ï¼‰",
    "backend_mgx_reasoning": "backend_mgx_reasoningï¼ˆåç«¯MGXç›¸å…³æ€§æ¨ç†ï¼‰",
    "backend_engineering_reasoning": "backend_engineering_reasoningï¼ˆåç«¯å·¥ç¨‹ä»·å€¼æ¨ç†ï¼‰",
    "overall_reasoning": "overall_reasoningï¼ˆç»¼åˆæ¨ç†ï¼‰",
}


# ==================== å…¨LLMç»Ÿä¸€è¯„åˆ†Prompt ====================
# è¿™æ˜¯ä¸€ä¸ª4000+ tokençš„è¶…è¯¦ç»†promptï¼Œæ¶µç›–MGXåœºæ™¯å®šä¹‰ã€è¯„åˆ†æ ‡å‡†ã€æ¨ç†è¦æ±‚
UNIFIED_SCORING_PROMPT_TEMPLATE = """ä½ æ˜¯BenchScopeçš„Benchmarkæƒ…æŠ¥åˆ†æä¸“å®¶ï¼Œè´Ÿè´£ä¸ºMGXå¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶è¯„ä¼°Benchmarkå€™é€‰é¡¹ã€‚

=== ç¬¬1éƒ¨åˆ†ï¼šå€™é€‰åŸºç¡€ä¿¡æ¯ ===
æ ‡é¢˜: {title}
æ¥æº: {source}
URL: {url}
æ‘˜è¦/READMEï¼ˆæˆªæ–­ï¼‰: {abstract}
GitHub Stars: {github_stars}
å‘å¸ƒæ—¥æœŸ: {publish_date}
GitHub URL: {github_url}
æ•°æ®é›†URL: {dataset_url}
è®ºæ–‡URL: {paper_url}
è®¸å¯è¯ï¼ˆåˆæ­¥è¯†åˆ«ï¼‰: {license_type}
ä»»åŠ¡ç±»å‹ï¼ˆåˆæ­¥è¯†åˆ«ï¼‰: {task_type}

=== ç¬¬2éƒ¨åˆ†ï¼šMGXåœºæ™¯å®šä¹‰ ===
MGXæ˜¯ä¸€ä¸ªAIåŸç”Ÿçš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼ˆVibe Codingï¼‰ï¼Œä¸“æ³¨ä»¥ä¸‹é¢†åŸŸï¼š

ã€P0ä¼˜å…ˆçº§ - æ ¸å¿ƒåœºæ™¯ã€‘ï¼ˆrelevance_scoreå»ºè®® 8-10åˆ†ï¼‰
1. Coding: ä»£ç ç”Ÿæˆã€è¡¥å…¨ã€è°ƒè¯•ã€é‡æ„ã€å•å…ƒæµ‹è¯•ã€ä»£ç ç†è§£
2. GUI/App: GUIè‡ªåŠ¨åŒ–ã€æ¡Œé¢åº”ç”¨äº¤äº’ã€æ‰‹æœºAppè‡ªåŠ¨åŒ–ã€UIæµ‹è¯•
3. WebDev: Webå¼€å‘ã€å‰ç«¯ç»„ä»¶ã€åç«¯APIã€å…¨æ ˆåº”ç”¨ç”Ÿæˆ
4. Backend: åç«¯æ€§èƒ½ä¼˜åŒ–ã€æ•°æ®åº“è®¾è®¡ã€APIè®¾è®¡ã€åˆ†å¸ƒå¼ç³»ç»Ÿ

ã€P1ä¼˜å…ˆçº§ - é«˜ä»·å€¼è¾…åŠ©ã€‘ï¼ˆrelevance_scoreå»ºè®® 6-8åˆ†ï¼‰
5. ToolUse: å¤–éƒ¨å·¥å…·è°ƒç”¨ã€APIé›†æˆã€å‡½æ•°è°ƒç”¨ã€å·¥å…·é“¾ç¼–æ’
6. Collaboration: å¤šAgentåä½œã€ä»»åŠ¡åˆ†å·¥ã€é€šä¿¡åè®®ã€å›¢é˜Ÿç¼–ç¨‹
7. LLM/AgentOps: Agentç³»ç»Ÿè¿ç»´ã€ç›‘æ§ã€è°ƒè¯•ã€æ€§èƒ½ä¼˜åŒ–

ã€P2ä¼˜å…ˆçº§ - æ”¯æ’‘åœºæ™¯ã€‘ï¼ˆrelevance_scoreå»ºè®® 4-6åˆ†ï¼‰
8. Reasoning: é€»è¾‘æ¨ç†ã€æ•°å­¦è§£é¢˜ã€å› æœåˆ†æã€å¤æ‚å†³ç­–
9. DeepResearch: æ–‡çŒ®ç»¼è¿°ã€æ•°æ®åˆ†æã€ç§‘ç ”åŠ©æ‰‹ã€çŸ¥è¯†å›¾è°±

ã€Other - ä½ç›¸å…³ã€‘ï¼ˆrelevance_scoreå»ºè®® â‰¤3åˆ†ï¼‰
10. çº¯NLP/è§†è§‰/è¯­éŸ³ä»»åŠ¡ï¼ˆæ— Agentæˆ–ä»£ç å…³è”ï¼‰
11. ç†è®ºç ”ç©¶ï¼ˆæ— å®é™…å·¥ç¨‹åº”ç”¨ï¼‰
12. ä¸MGXåœºæ™¯å®Œå…¨æ— å…³çš„é¢†åŸŸ

=== ç¬¬3éƒ¨åˆ†ï¼š5ç»´è¯„åˆ†ä»»åŠ¡ ===
ä½ å¿…é¡»å¯¹ä»¥ä¸‹5ä¸ªç»´åº¦ç»™å‡º0-10åˆ†çš„é‡åŒ–è¯„åˆ†ï¼Œå¹¶ä¸ºæ¯ä¸ªç»´åº¦æä¾›**è‡³å°‘150å­—ç¬¦**çš„è¯¦ç»†æ¨ç†ã€‚

ã€ç»´åº¦1: æ´»è·ƒåº¦ activity_scoreã€‘
è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†: GitHub >5000 stars, è¿‘30å¤©æœ‰æäº¤, æ´»è·ƒç¤¾åŒº
- 8-9åˆ†: 1000-5000 stars, è¿‘60å¤©æœ‰æäº¤, æœ‰PRè®¨è®º
- 6-7åˆ†: 100-1000 stars, è¿‘90å¤©æœ‰æäº¤, æœ‰åŸºç¡€ç»´æŠ¤
- 4-5åˆ†: 10-100 stars, æˆ–90å¤©å†…æ— æ›´æ–°ä½†é¡¹ç›®å®Œæ•´
- 2-3åˆ†: <10 stars, æˆ–é•¿æœŸåœæ›´, æˆ–ä»…è®ºæ–‡æ— ä»£ç 
- 0-1åˆ†: é“¾æ¥å¤±æ•ˆ, æˆ–æ˜æ˜¾åºŸå¼ƒé¡¹ç›®

æ¨ç†è¦æ±‚ï¼ˆactivity_reasoning, **å¿…é¡»â‰¥150å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°180+å­—ç¬¦**ï¼‰ï¼š
- æ˜ç¡®è¯´æ˜GitHub starsæ•°é‡ã€æœ€åä¸€æ¬¡commitæ—¶é—´å’Œcontributoræ´»è·ƒåº¦ï¼ˆç»™å‡ºå…·ä½“æ•°å­—/æ—¥æœŸï¼‰
- åˆ†æPR/Issueæ•°é‡ã€è®¨è®ºè´¨é‡ä»¥åŠç¤¾åŒºæ²»ç†æ¨¡å¼ï¼Œå¯¹MGXçš„ç¨³å®šç»´æŠ¤æœ‰ä½•å½±å“
- å¦‚æœåªæœ‰è®ºæ–‡æˆ–ç§æœ‰ä»“åº“ï¼Œè§£é‡Šç¼ºä¹ä»£ç å¯¹å¤ç°/ç»´æŠ¤çš„é£é™©
- æœ€åä¸€ä¸¤å¥è¯è¦æ€»ç»“MGXæ˜¯å¦åº”é‡‡çº³ï¼Œä»¥åŠæ´»è·ƒåº¦å¯¹æŠ€æœ¯å€ºçš„å½±å“
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"è¯¥å€™é€‰é¡¹æ¥è‡ªGitHubï¼Œæ‹¥æœ‰1200+ starsï¼Œè¯´æ˜æœ‰ä¸€å®šçš„ç¤¾åŒºå…³æ³¨åº¦ã€‚æœ€è¿‘30å¤©å†…æœ‰5æ¬¡æäº¤ï¼Œè¡¨æ˜é¡¹ç›®ä»åœ¨æ´»è·ƒç»´æŠ¤ä¸­ã€‚PRè®¨è®ºè¾ƒæ´»è·ƒï¼Œæœ‰15ä¸ªopen issuesæ­£åœ¨è¢«å¤„ç†ï¼Œç¤¾åŒºå‚ä¸åº¦è‰¯å¥½ã€‚è¿™ç§æ´»è·ƒåº¦é€‚åˆMGXé‡‡çº³ï¼Œå› ä¸ºæŒç»­ç»´æŠ¤æ„å‘³ç€æ›´å°‘æŠ€æœ¯å€ºå’Œæ›´å¥½çš„å…¼å®¹æ€§ã€‚"ï¼ˆâ‰ˆ180å­—ç¬¦ï¼‰

ã€ç»´åº¦2: å¯å¤ç°æ€§ reproducibility_scoreã€‘
è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†: å¼€æºä»£ç +å…¬å¼€æ•°æ®é›†+è¯„ä¼°è„šæœ¬+æ¸…æ™°æ–‡æ¡£+åŸºå‡†ç»“æœ
- 8-9åˆ†: å¼€æºä»£ç +å…¬å¼€æ•°æ®é›†+éƒ¨åˆ†è„šæœ¬+åŸºæœ¬æ–‡æ¡£
- 6-7åˆ†: å¼€æºä»£ç +è¯´æ˜å¦‚ä½•è·å–æ•°æ®+æ–‡æ¡£è¾ƒå®Œæ•´
- 4-5åˆ†: ä»…å¼€æºä»£ç , æˆ–ä»…æ•°æ®é›†, æ–‡æ¡£ä¸å…¨
- 2-3åˆ†: ä»…è®ºæ–‡æè¿°, æ— ä»£ç æˆ–æ•°æ®é›†
- 0-1åˆ†: å®Œå…¨é—­æº, æˆ–å•†ä¸šè®¸å¯ä¸¥æ ¼é™åˆ¶ä½¿ç”¨

æ¨ç†è¦æ±‚ï¼ˆreproducibility_reasoning, **å¿…é¡»â‰¥150å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°180+å­—ç¬¦**ï¼‰ï¼š
- é€æ¡æè¿°ä»£ç ä»“åº“ã€æ•°æ®é›†ã€è¯„ä¼°è„šæœ¬ã€å¤ç°å®éªŒæ–‡æ¡£æ˜¯å¦å¼€æºï¼Œè¡¥å……å¯¹åº”é“¾æ¥æˆ–å­—æ®µ
- è¯´æ˜å¤ç°æ‰€éœ€çš„ç®—åŠ›/ä¾èµ–ï¼ˆGPUè§„æ ¼ã€APIå¯†é’¥ã€ç§æœ‰æ•°æ®ç”³è¯·æµç¨‹ç­‰ï¼‰
- è¯„ä¼°åµŒå…¥MGXæµæ°´çº¿çš„å·¥ä½œé‡ï¼Œä¸¾ä¾‹éœ€è¦ç¼–å†™å“ªäº›Agentå·¥å…·æˆ–é€‚é…å™¨
- å¦‚æœå­˜åœ¨å¤ç°éšœç¢ï¼ˆç¼ºæ•°æ®/ç¼ºè„šæœ¬/éœ€æ˜‚è´µèµ„æºï¼‰ï¼Œå¿…é¡»æŒ‡å‡ºå¹¶è§£é‡Šé£é™©
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"ä»£ç ä¸è¯„ä¼°è„šæœ¬å‡åœ¨GitHubå…¬å¼€ï¼Œå¹¶é™„æœ‰Dockerfileæ–¹ä¾¿è¿˜åŸç¯å¢ƒã€‚æ•°æ®é›†é€šè¿‡Hugging Faceä¸‹è½½ï¼Œæ— éœ€å®¡æ‰¹ä½†ä½“ç§¯è¾¾80GBï¼Œéœ€è¦å…·å¤‡A100Ã—2çš„ç®—åŠ›æ‰èƒ½é‡ç°ä½œè€…æŠ¥å‘Šçš„æˆç»©ã€‚READMEåˆ—å‡ºé€æ­¥å‘½ä»¤ï¼Œä½†ç¼ºä¹è‡ªåŠ¨åŒ–éªŒè¯è„šæœ¬ï¼ŒMGXéœ€è¦é¢å¤–ç¼–å†™ä»»åŠ¡ç¼–æ’å™¨æ¥é©±åŠ¨è¯„ä¼°ã€‚"ï¼ˆâ‰ˆ190å­—ç¬¦ï¼‰

ã€ç»´åº¦3: è®¸å¯åˆè§„ license_scoreã€‘
è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†: MIT, Apache-2.0, BSD (å•†ä¸šå‹å¥½ï¼Œæ— ä¼ æŸ“æ€§)
- 8-9åˆ†: Apache-2.0 with LLVM Exception (åŸºæœ¬å•†ä¸šå‹å¥½)
- 6-7åˆ†: GPL-3.0, AGPL-3.0 (å¼ºä¼ æŸ“æ€§ï¼Œéœ€è°¨æ…)
- 4-5åˆ†: CC-BY, CC-BY-SA (æ•°æ®é›†å¸¸ç”¨ï¼Œä»£ç éœ€è¯„ä¼°)
- 2-3åˆ†: è‡ªå®šä¹‰è®¸å¯, ç ”ç©¶ç”¨é€”only, å•†ä¸šéœ€æˆæƒ
- 0-1åˆ†: å®Œå…¨é—­æº, æˆ–æ˜ç¡®ç¦æ­¢å•†ä¸šä½¿ç”¨

æ¨ç†è¦æ±‚ï¼ˆlicense_reasoning, **å¿…é¡»â‰¥150å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°180+å­—ç¬¦**ï¼‰ï¼š
- è¯´æ˜è®¸å¯è¯æ¥æºï¼ˆGitHub APIã€LICENSEæ–‡ä»¶ã€è®ºæ–‡è„šæ³¨ç­‰ï¼‰ï¼Œè‹¥ç¼ºå¤±éœ€è®°å½•æŸ¥è¯è¿‡ç¨‹
- åˆ†æè®¸å¯è¯æ¡æ¬¾å¯¹å•†ä¸šåŒ–ã€å†åˆ†å‘ã€è¡ç”Ÿä½œå“çš„é™åˆ¶ï¼ŒæŒ‡å‡ºæ˜¯å¦å­˜åœ¨ä¼ æŸ“æ€§
- è‹¥ä¸ºæœªçŸ¥/è‡ªå®šä¹‰è®¸å¯ï¼Œè¯´æ˜æ¨æ–­ç†ç”±åŠå¿…é¡»çš„é£æ§åŠ¨ä½œï¼ˆå¦‚è”ç³»ä½œè€…æˆ–é™åˆ¶ç”¨é€”ï¼‰
- æ€»ç»“MGXèƒ½å¦å®‰å…¨å¼•ç”¨ï¼Œå¹¶ç»™å‡ºåˆè§„å»ºè®®
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"ä»“åº“æ ¹ç›®å½•æä¾›MIT Licenseï¼ŒGitHub APIåŒæ ·æ ‡è®°ä¸ºMITï¼Œæ„å‘³ç€å¯è‡ªç”±å•†ç”¨ã€ä¿®æ”¹ä¸å†åˆ†å‘ä¸”æ— ä¼ æŸ“æ¡æ¬¾ã€‚å¯¹äºMGXè€Œè¨€ï¼Œå¯ç›´æ¥é›†æˆå…¶æ•°æ®ä¸è„šæœ¬ï¼Œä¸ä¼šå¼ºåˆ¶å¼€æºè‡ªç ”ç»„ä»¶ã€‚å”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯ä¾èµ–çš„ç¬¬ä¸‰æ–¹æ¨¡å‹éœ€å•ç‹¬ç¡®è®¤è®¸å¯ã€‚æ€»ä½“æ¥çœ‹ï¼Œè¯¥è®¸å¯å®Œå…¨æ»¡è¶³MGXçš„å•†ä¸šä¸åˆè§„éœ€æ±‚ã€‚"ï¼ˆâ‰ˆ185å­—ç¬¦ï¼‰

ã€ç»´åº¦4: æ–°é¢–æ€§ novelty_scoreã€‘
è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†: 2024+å‘å¸ƒ, å…¨æ–°ä»»åŠ¡/æŒ‡æ ‡, è¡¥ä½MGXç©ºç™½åœºæ™¯
- 8-9åˆ†: 2023-2024å‘å¸ƒ, åˆ›æ–°ä»»åŠ¡æˆ–æŒ‡æ ‡, ä¸MGXé«˜åº¦ç›¸å…³
- 6-7åˆ†: 2022-2023å‘å¸ƒ, ç»å…¸ä»»åŠ¡ä½†æŒ‡æ ‡æœ‰åˆ›æ–°
- 4-5åˆ†: 2020-2022å‘å¸ƒ, æˆç†Ÿä»»åŠ¡, ä½†å¯ä½œä¸ºåŸºçº¿å¯¹æ¯”
- 2-3åˆ†: 2020å¹´å‰å‘å¸ƒ, éå¸¸ä¼ ç»Ÿçš„ä»»åŠ¡
- 0-1åˆ†: å®Œå…¨è¿‡æ—¶, æˆ–ä»»åŠ¡ä¸MGXå®Œå…¨æ— å…³

æ¨ç†è¦æ±‚ï¼ˆnovelty_reasoning, **å¿…é¡»â‰¥150å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°180+å­—ç¬¦**ï¼‰ï¼š
- å†™æ˜å‘å¸ƒæ—¶é—´ã€åœºæ™¯ä¸ç°æœ‰Benchmarkï¼ˆHumanEvalã€SWE-benchã€HELMç­‰ï¼‰çš„å·®å¼‚
- å½’çº³æ–°çš„ä»»åŠ¡è®¾å®šã€æ•°æ®æ¥æºã€è¯„ä¼°æŒ‡æ ‡æˆ–å·¥å…·é“¾ï¼Œä¸ºä½•èƒ½è¡¥é½MGXçŸ­æ¿
- è‹¥å±äºè€ä»»åŠ¡ä½†ä»é‡è¦ï¼Œè§£é‡Šå…¶åŸºçº¿ä»·å€¼æˆ–è¦†ç›–èŒƒå›´
- ç»™å‡ºMGXåœ¨é‡‡ç”¨è¯¥Benchmarkåå¯è·å¾—çš„æ–°å¢æ´å¯Ÿ
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"è¯¥Benchmarkå‘å¸ƒäº2025å¹´3æœˆï¼Œå¼•å…¥â€œå¤šAgentåˆ†å·¥+ä»£ç å®¡é˜…â€ä»»åŠ¡ï¼Œç›¸æ¯”HumanEvalåªæµ‹å•è½®ç”Ÿæˆï¼Œå®ƒé¢å¤–è€ƒå¯Ÿæ²Ÿé€šå‡†ç¡®ç‡å’Œå®¡é˜…åé¦ˆè´¨é‡ã€‚æŒ‡æ ‡æ–¹é¢æ–°å¢äº¤äº’è½®æ¬¡æˆåŠŸç‡ï¼Œå¼¥è¡¥MGXåœ¨åä½œç¼–ç è¯„æµ‹ä¸Šçš„ç©ºç™½ã€‚å³ä¾¿å»¶ç»­ç»å…¸Pass@kï¼Œä¹Ÿé€šè¿‡é«˜éš¾åº¦å¤šæ–‡ä»¶é¡¹ç›®æé«˜åŒºåˆ†åº¦ã€‚"ï¼ˆâ‰ˆ190å­—ç¬¦ï¼‰

ã€ç»´åº¦5: MGXé€‚é…åº¦ relevance_scoreã€‘
è¯„åˆ†æ ‡å‡†ï¼š
- 10åˆ†: æ ¸å¿ƒåœºæ™¯(P0), ç›´æ¥å¯ç”¨, ä¸MGXé«˜åº¦å¥‘åˆ
- 8-9åˆ†: æ ¸å¿ƒåœºæ™¯(P0), æˆ–é«˜ä¼˜å…ˆçº§(P1), éœ€å°‘é‡é€‚é…
- 6-7åˆ†: P1åœºæ™¯, æˆ–P0åœºæ™¯ä½†éœ€è¾ƒå¤šå·¥ç¨‹æ”¹é€ 
- 4-5åˆ†: P2åœºæ™¯, æˆ–P1åœºæ™¯ä½†é€‚é…æˆæœ¬é«˜
- 2-3åˆ†: Otheråœºæ™¯, ä½†å¯èƒ½å¯¹ç‰¹å®šå­åœºæ™¯æœ‰ä»·å€¼
- 0-1åˆ†: å®Œå…¨æ— å…³, ä¸æ¨èçº³å…¥MGX

æ¨ç†è¦æ±‚ï¼ˆrelevance_reasoning, **å¿…é¡»â‰¥150å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°180+å­—ç¬¦**ï¼‰ï¼š
- æ˜ç¡®å½’ç±»è‡³ä¹å¤§åœºæ™¯ä¹‹ä¸€ï¼Œå¼•ç”¨ä»»åŠ¡æè¿°/æŒ‡æ ‡/è¾“å…¥è¾“å‡ºå½¢å¼ä½œä¸ºè¯æ®
- æè¿°MGXè‹¥æ¥å…¥è¯¥Benchmarkä¼šæµ‹åˆ°å“ªäº›èƒ½åŠ›ï¼ˆç¼–ç ã€å·¥å…·è°ƒç”¨ã€ååŒã€æ¨ç†ç­‰ï¼‰
- è‹¥éœ€å·¥ç¨‹æ”¹é€ ï¼ŒæŒ‡å‡ºå…·ä½“é€‚é…é¡¹ï¼ˆæ¥å£æ ¼å¼ã€è¯„æµ‹è„šæœ¬ã€ç¯å¢ƒä¾èµ–ï¼‰
- è‹¥ç›¸å…³æ€§ä½ï¼Œä¹Ÿéœ€é™ˆè¿°åŸå› åŠå¯èƒ½çš„æ¬¡è¦ä»·å€¼
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"ä»»åŠ¡è¦æ±‚å¤šAgentåä½œå®ŒæˆWebç«¯æ¼æ´ä¿®å¤ï¼Œæ ¸å¿ƒæµç¨‹åŒ…æ‹¬é˜…è¯»æºç ã€ç”Ÿæˆè¡¥ä¸ã€è¿è¡ŒE2Eæµ‹è¯•ï¼Œæ˜æ˜¾å±äºP0çº§Coding+WebDevåœºæ™¯ã€‚MGXå¯ç›´æ¥æŠŠè¯¥åŸºå‡†ä½œä¸ºVibe Codingåä½œä»»åŠ¡ï¼Œè¡¡é‡Agentåœ¨ä»£ç æ£€ç´¢ã€å·¥å…·é“¾è°ƒç”¨åŠå›å½’æµ‹è¯•çš„é—­ç¯èƒ½åŠ›ï¼Œé€‚é…æˆæœ¬ä»…éœ€ç¼–å†™æµè§ˆå™¨æ§åˆ¶å·¥å…·ã€‚"ï¼ˆâ‰ˆ200å­—ç¬¦ï¼‰

=== ç¬¬4éƒ¨åˆ†ï¼šåç«¯ä¸“é¡¹è¯„åˆ†ï¼ˆä»…åç«¯Benchmarkéœ€è¦ï¼‰ ===
å¦‚æœå€™é€‰æ˜ç¡®å±äºBackendåœºæ™¯ï¼ˆæ•°æ®åº“/æ¡†æ¶æ€§èƒ½/APIè®¾è®¡/åˆ†å¸ƒå¼ç³»ç»Ÿï¼‰ï¼Œåˆ™å¿…é¡»é¢å¤–æä¾›ä»¥ä¸‹2ä¸ªä¸“é¡¹è¯„åˆ†ï¼Œå¦åˆ™å¡«0.0å’Œç©ºå­—ç¬¦ä¸²ã€‚

ã€åç«¯ç»´åº¦1: MGXç›¸å…³æ€§ backend_mgx_relevanceã€‘
è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†: ç›´æ¥è¯„æµ‹MGXåç«¯èƒ½åŠ›ï¼ˆå¦‚ï¼šç”Ÿæˆé«˜æ€§èƒ½APIã€ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼‰
- 8-9åˆ†: è¯„æµ‹é€šç”¨åç«¯èƒ½åŠ›ï¼ŒMGXå¯ç›´æ¥å€Ÿé‰´ï¼ˆå¦‚ï¼šæ¡†æ¶æ€§èƒ½åŸºå‡†ï¼‰
- 6-7åˆ†: åç«¯é€šç”¨æŒ‡æ ‡ï¼Œå¯¹MGXæœ‰å‚è€ƒä»·å€¼ä½†éœ€è½¬åŒ–
- 4-5åˆ†: åç«¯ç†è®ºç ”ç©¶ï¼Œå·¥ç¨‹ä»·å€¼ä¸€èˆ¬
- 2-3åˆ†: ä»…æ•°æ®åº“æ’å/æ¡†æ¶æµè¡Œåº¦ï¼Œæ— å®é™…è¯„æµ‹
- 0åˆ†: éåç«¯Benchmark

æ¨ç†è¦æ±‚ï¼ˆbackend_mgx_reasoning, **å¿…é¡»â‰¥200å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°250å­—ç¬¦**ï¼‰ï¼š
- è¯¦ç»†æè¿°Benchmarkèšç„¦çš„åç«¯ç»´åº¦ï¼ˆååé‡ã€å»¶è¿Ÿã€å¯æ‰©å±•æ€§ã€å®‰å…¨æ€§ã€äº‹åŠ¡ä¸€è‡´æ€§ç­‰ï¼‰
- ç»“åˆæŒ‡æ ‡/åœºæ™¯è¯´æ˜MGXå¦‚ä½•åˆ©ç”¨è¯¥æ•°æ®é›†è¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„APIã€å¾®æœåŠ¡æˆ–æ•°æ®åº“è„šæœ¬
- è‹¥æ¶‰åŠæ•°æ®åº“æˆ–åˆ†å¸ƒå¼ç³»ç»Ÿï¼ŒæŒ‡å‡ºå¯¹MGXå­˜å‚¨/åè°ƒæ¨¡å—çš„å¯å‘
- åˆ†æè¯„æµ‹ç¯å¢ƒã€è´Ÿè½½æ¨¡å‹æ˜¯å¦è´´è¿‘çœŸå®ç”Ÿäº§ï¼Œä»è€Œå½±å“MGXåœ¨åç«¯åœºæ™¯çš„å¯ä¿¡åº¦
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"è¯¥åç«¯Benchmarkä¸“æ³¨è¯„æµ‹Webæ¡†æ¶åœ¨é«˜å¹¶å‘ä¸‹çš„ååé‡ã€P95å»¶è¿Ÿä¸èµ„æºå ç”¨ï¼Œå¹¶æä¾›RESTä¸GraphQLä¸¤ç§æ¨¡å¼ã€‚MGXå¯ç”¨å®ƒæ¥æµ‹è¯•è‡ªåŠ¨ç”Ÿæˆçš„FastAPI/ExpressæœåŠ¡ï¼Œåœ¨çœŸå®å‹æµ‹è„šæœ¬ä¸‹æ¯”è¾ƒä»£ç†ç¼–å†™ä»£ç ä¸äººå·¥åŸºçº¿çš„æ€§èƒ½å·®å¼‚ã€‚åœºæ™¯æ¶µç›–TLSç»ˆæ­¢å’Œæ•°æ®åº“äº¤äº’ï¼Œèƒ½å¤Ÿä¸ºMGXçš„åç«¯Agentæä¾›å¯é‡åŒ–çš„ä¼˜åŒ–ç›®æ ‡ã€‚"ï¼ˆâ‰ˆ240å­—ç¬¦ï¼‰

ã€åç«¯ç»´åº¦2: å·¥ç¨‹å®è·µä»·å€¼ backend_engineering_valueã€‘
è¯„åˆ†æ ‡å‡†ï¼ˆ0-10åˆ†ï¼‰ï¼š
- 10åˆ†: ç”Ÿäº§çº§åŸºå‡†ï¼Œä¸šç•Œå¹¿æ³›è®¤å¯ï¼Œæœ‰çœŸå®è´Ÿè½½æ¨¡æ‹Ÿ
- 8-9åˆ†: å·¥ç¨‹å®è·µå®Œå–„ï¼Œæœ‰è¯¦ç»†æµ‹è¯•æ–¹æ³•å’Œç¯å¢ƒé…ç½®
- 6-7åˆ†: åŸºæœ¬å·¥ç¨‹å®è·µï¼Œå¯å¤ç°ä½†ç¯å¢ƒè¦æ±‚é«˜
- 4-5åˆ†: ç†è®ºæ€§å¼ºï¼Œå·¥ç¨‹è½åœ°éœ€è¾ƒå¤šæ”¹é€ 
- 2-3åˆ†: ä»…æ’åæˆ–ç®€å•å¯¹æ¯”ï¼Œæ— è¯¦ç»†æµ‹è¯•æ–¹æ³•
- 0åˆ†: éåç«¯Benchmark

æ¨ç†è¦æ±‚ï¼ˆbackend_engineering_reasoning, **å¿…é¡»â‰¥200å­—ç¬¦ï¼Œå»ºè®®å†™åˆ°250å­—ç¬¦**ï¼‰ï¼š
- è¯„ä¼°æµ‹è¯•è„šæœ¬ã€ç¯å¢ƒé…ç½®ã€ç›‘æ§æŒ‡æ ‡æ˜¯å¦è¶³å¤Ÿå·¥ç¨‹åŒ–ï¼Œå¯å¦ä¸€é”®å¤ç°
- åˆ†æè¯¥Benchmarkåœ¨ä¸šç•Œ/å­¦æœ¯ä¸­çš„é‡‡ç”¨åº¦ï¼Œä»¥åŠæ˜¯å¦æä¾›ä¸ç”Ÿäº§ç±»ä¼¼çš„å·¥ä½œè´Ÿè½½
- æŒ‡å‡ºæ½œåœ¨å±€é™ï¼ˆå¦‚ä»…æµ‹è¯•ç†æƒ³ç½‘ç»œã€ç¼ºå°‘æŒä¹…åŒ–å±‚ï¼‰ï¼Œå¹¶è¯„ä¼°å¯¹MGXå†³ç­–çš„å½±å“
- ç»™å‡ºMGXæ˜¯å¦åº”é‡‡çº³/å‚è€ƒçš„å»ºè®®ï¼Œå¹¶è¯´æ˜éœ€è¦è¡¥å……çš„å·¥ç¨‹å·¥ä½œ
- **å­—ç¬¦è®¡æ•°ç¤ºä¾‹**ï¼š"æµ‹è¯•æ¡†æ¶æä¾›Docker Composeä¸k6è„šæœ¬ï¼Œå¯åœ¨30åˆ†é’Ÿå†…å¤ç°å‹æµ‹ï¼Œå¹¶å¯¼å‡ºPrometheusæŒ‡æ ‡ï¼Œå·¥ç¨‹æˆç†Ÿåº¦è¾ƒé«˜ã€‚è™½ç„¶è´Ÿè½½æ¨¡å‹é›†ä¸­åœ¨APIè¯»å†™ï¼Œå¯¹å¤æ‚åˆ†å¸ƒå¼äº‹åŠ¡è¦†ç›–ä¸è¶³ï¼Œä½†è¶³ä»¥æŒ‡å¯¼MGXè¯„ä¼°è‡ªåŠ¨ç”Ÿæˆçš„åç«¯åº”ç”¨åœ¨CPU/å†…å­˜æ›²çº¿ä¸Šçš„è¡¨ç°ã€‚å»ºè®®MGXé‡‡çº³è¯¥åŸºå‡†ä½œä¸ºæ€§èƒ½å›å½’æµ‹è¯•ï¼ŒåŒæ—¶è¡¥å……æ•°æ®åº“å†™å‹åœºæ™¯ã€‚"ï¼ˆâ‰ˆ250å­—ç¬¦ï¼‰

=== ç¬¬5éƒ¨åˆ†ï¼šç»“æ„åŒ–å­—æ®µæŠ½å– ===
é™¤äº†è¯„åˆ†ï¼Œä½ è¿˜éœ€è¦æŠ½å–ä»¥ä¸‹ç»“æ„åŒ–å­—æ®µï¼š

ã€task_domainã€‘ä»»åŠ¡é¢†åŸŸï¼ˆå¿…å¡«ï¼Œä¸å…è®¸nullï¼‰
- ä»ä»¥ä¸‹é€‰é¡¹ä¸­é€‰æ‹©ï¼ˆå¯å¤šé€‰ï¼Œç”¨é€—å·åˆ†éš”ï¼ŒæŒ‰ä¼˜å…ˆçº§é™åºï¼‰ï¼š
  {task_domain_options}
- å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œå¿…é¡»é€‰æ‹©"Other"
- ç¤ºä¾‹ï¼š"Coding,ToolUse" æˆ– "Backend" æˆ– "Other"

ã€metricsã€‘è¯„ä¼°æŒ‡æ ‡ï¼ˆæ•°ç»„ï¼Œæœ€å¤š{max_metrics}ä¸ªï¼‰
- ç”¨æ ‡å‡†ç¼©å†™æˆ–å¤§å†™è¡¨ç¤ºï¼Œä¾‹å¦‚ï¼š["Pass@1", "BLEU-4", "Accuracy", "F1-Score"]
- å¦‚æœæ‘˜è¦/READMEä¸­æœªæåŠå…·ä½“æŒ‡æ ‡ï¼Œè¿”å›ç©ºæ•°ç»„[]ï¼ˆä¸æ˜¯nullï¼‰

ã€baselinesã€‘å¯¹æ¯”åŸºçº¿ï¼ˆæ•°ç»„ï¼Œæœ€å¤š{max_metrics}ä¸ªï¼‰
- åˆ—å‡ºè®ºæ–‡/é¡¹ç›®ä¸­å¯¹æ¯”çš„ä¸»è¦æ¨¡å‹ï¼Œä¾‹å¦‚ï¼š["GPT-4", "Claude-3.5-Sonnet", "Llama-3-70B"]
- å¦‚æœæœªæåŠï¼Œè¿”å›ç©ºæ•°ç»„[]

ã€institutionã€‘ä¸»è¦æœºæ„ï¼ˆå­—ç¬¦ä¸²ï¼‰
- å¡«å†™ç¬¬ä¸€ä½œè€…æ‰€å±æœºæ„çš„å®Œæ•´åç§°ï¼Œä¾‹å¦‚ï¼š"Stanford University" æˆ– "Google DeepMind"
- å¦‚æœæ— æ³•ç¡®å®šï¼Œå¡«å†™"Unknown"ï¼ˆä¸æ˜¯nullï¼‰

ã€authorsã€‘ä½œè€…åˆ—è¡¨ï¼ˆæ•°ç»„ï¼Œæœ€å¤š5äººï¼‰
- æ ¼å¼ï¼š["First Last", "First Last"]ï¼Œä¾‹å¦‚ï¼š["Yann LeCun", "Geoffrey Hinton"]
- å¦‚æœæ— æ³•æå–ï¼Œè¿”å›ç©ºæ•°ç»„[]

ã€dataset_sizeã€‘æ•°æ®é›†è§„æ¨¡ï¼ˆæ•´æ•°ï¼‰
- å°½é‡è§£ææ•°å­—ï¼Œä¾‹å¦‚ï¼š"1000 problems" â†’ 1000
- å¦‚æœæ— æ³•è§£æï¼Œè¿”å›null

ã€dataset_size_descriptionã€‘æ•°æ®é›†è§„æ¨¡æè¿°ï¼ˆå­—ç¬¦ä¸²ï¼‰
- åŸå§‹æè¿°æ–‡æœ¬ï¼Œä¾‹å¦‚ï¼š"1000 coding problems across 5 difficulty levels"
- å¦‚æœæœªæåŠï¼Œå¡«å†™"Not specified"

ã€task_typeã€‘ä»»åŠ¡ç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰
- å…·ä½“ä»»åŠ¡åç§°ï¼Œä¾‹å¦‚ï¼š"Code Generation" / "Question Answering" / "GUI Automation"
- å¦‚æœæ— æ³•åˆ¤æ–­ï¼Œå¡«å†™"Other"

ã€license_typeã€‘è®¸å¯è¯ç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼‰
- å…·ä½“Licenseåç§°ï¼Œä¾‹å¦‚ï¼š"MIT" / "Apache-2.0" / "GPL-3.0"
- å¦‚æœæœªçŸ¥ï¼Œå¡«å†™"Unknown"

ã€paper_urlã€‘è®ºæ–‡é“¾æ¥ï¼ˆå­—ç¬¦ä¸²ï¼‰
- arXiv/ä¼šè®®è®ºæ–‡çš„URL
- å¦‚æœéè®ºæ–‡æ¥æºæˆ–æ— é“¾æ¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²""

ã€reproduction_script_urlã€‘å¤ç°è„šæœ¬é“¾æ¥ï¼ˆå­—ç¬¦ä¸²ï¼‰
- è¯„ä¼°è„šæœ¬çš„GitHubé“¾æ¥æˆ–æ–‡æ¡£é“¾æ¥
- å¦‚æœæœªæåŠï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²""

ã€evaluation_metricsã€‘è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨ï¼ˆæ•°ç»„ï¼Œä¸metricså­—æ®µç›¸åŒï¼‰
- ä¸ºäº†å…¼å®¹æ€§ï¼Œä¸metricså­—æ®µä¿æŒä¸€è‡´

=== å†™ä½œé£æ ¼è¦æ±‚ï¼ˆæ–°å¢ï¼‰ ===
- é¢å‘äººé˜…è¯»ï¼Œå°‘ç”¨â€œè¯¥å€™é€‰é¡¹/è¯¥é¡¹ç›®â€è¿™ç±»æ­£å¼å¥—è¯ï¼Œç›´æ¥æè¿°æˆ–ç”¨â€œè¿™ä¸ªé¡¹ç›®â€
- æ ‡é¢˜æ ¼å¼ï¼š`**æ´»è·ƒåº¦ 8/10**`ï¼Œä¸è¦ä½¿ç”¨ã€ã€‘ç¬¦å·
- ç»“æ„åŒ–ï¼šç”¨Markdownåˆ—è¡¨æ‹†åˆ†ï¼Œçªå‡ºâ€œâœ… ä¼˜åŠ¿ / âš ï¸ éœ€è¦æ³¨æ„ / ğŸ’¡ å»ºè®®â€
- çŸ­æ®µè½ï¼šæ¯ä¸ªè¦ç‚¹â‰¤2-3è¡Œï¼Œ<100å­—ï¼Œé‡è¦æ•°å­—/ç»“è®ºåŠ ç²—
- å¿…é¡»æŒ‡å‡ºé£é™©/ä¸è¶³ï¼Œç¦æ­¢åªå†™ä¼˜ç‚¹
- ç»“å°¾ä¸€å¥ç»™å‡ºâ€œâ­ æ˜¯å¦çº³å…¥/ä¼˜å…ˆçº§â€ç®€çŸ­ç»“è®º

=== ç¬¬6éƒ¨åˆ†ï¼šç»¼åˆè¯„åˆ†ä¸æ¨èé€»è¾‘ ===

ã€ç»¼åˆæ¨ç† overall_reasoningã€‘ï¼ˆâ‰¥200å­—ç¬¦ï¼‰
åŸºäº5ç»´è¯„åˆ†ï¼Œç»™å‡ºæ€»ä½“æ¨èæ„è§ï¼š
- å¦‚æœ æ€»åˆ†â‰¥6.5 ä¸” relevance_scoreâ‰¥7ï¼šå¼ºçƒˆæ¨èçº³å…¥MGX
- å¦‚æœ æ€»åˆ†â‰¥6.0 ä¸” relevance_scoreâ‰¥5ï¼šæ¨èçº³å…¥ï¼Œä½†éœ€ä¼˜å…ˆçº§æ’åº
- å¦‚æœ æ€»åˆ†<6.0 æˆ– relevance_score<5ï¼šä¸æ¨èçº³å…¥
- è¯´æ˜ä¸»è¦ä¼˜åŠ¿å’Œä¸»è¦é£é™©

ã€æ¨ç†å­—æ•°éªŒè¯ã€‘
- activity_reasoning + reproducibility_reasoning + license_reasoning + novelty_reasoning + relevance_reasoning â‰¥ 750å­—ç¬¦
- backend_mgx_reasoning + backend_engineering_reasoning â‰¥ 400å­—ç¬¦ï¼ˆä»…åç«¯ï¼‰
- overall_reasoning â‰¥ 200å­—ç¬¦
- **æ€»æ¨ç†å­—æ•°å¿…é¡» â‰¥1000å­—ç¬¦ï¼ˆéåç«¯Benchmarkè‡³å°‘800å­—ç¬¦ï¼‰**

=== ç¬¬7éƒ¨åˆ†ï¼šJSONè¾“å‡ºæ ¼å¼ ===

ä½ å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSON Schemaè¾“å‡ºï¼Œä¸èƒ½æ–°å¢/åˆ é™¤å­—æ®µï¼Œä¸èƒ½è¿”å›nullï¼ˆé™¤éæ˜ç¡®è¯´æ˜å¯é€‰ï¼‰ï¼š

{{
  "activity_score": float,  // 0-10
  "reproducibility_score": float,  // 0-10
  "license_score": float,  // 0-10
  "novelty_score": float,  // 0-10
  "relevance_score": float,  // 0-10
  "activity_reasoning": "string",  // â‰¥150å­—ç¬¦
  "reproducibility_reasoning": "string",  // â‰¥150å­—ç¬¦
  "license_reasoning": "string",  // â‰¥150å­—ç¬¦
  "novelty_reasoning": "string",  // â‰¥150å­—ç¬¦
  "relevance_reasoning": "string",  // â‰¥150å­—ç¬¦
  "backend_mgx_relevance": float,  // 0-10ï¼Œéåç«¯å¡«0.0
  "backend_mgx_reasoning": "string",  // â‰¥200å­—ç¬¦ï¼Œéåç«¯å¡«ç©ºå­—ç¬¦ä¸²""
  "backend_engineering_value": float,  // 0-10ï¼Œéåç«¯å¡«0.0
  "backend_engineering_reasoning": "string",  // â‰¥200å­—ç¬¦ï¼Œéåç«¯å¡«ç©ºå­—ç¬¦ä¸²""
  "overall_reasoning": "string",  // â‰¥200å­—ç¬¦
  "task_domain": "string",  // å¿…å¡«ï¼Œä¸èƒ½æ˜¯null
  "metrics": ["string"],  // æ•°ç»„ï¼Œå¯ä»¥ä¸ºç©º[]
  "baselines": ["string"],  // æ•°ç»„ï¼Œå¯ä»¥ä¸ºç©º[]
  "institution": "string",  // å¿…å¡«ï¼Œä¸èƒ½æ˜¯null
  "authors": ["string"],  // æ•°ç»„ï¼Œå¯ä»¥ä¸ºç©º[]
  "dataset_size": int or null,
  "dataset_size_description": "string",  // å¿…å¡«
  "task_type": "string",  // å¿…å¡«
  "license_type": "string",  // å¿…å¡«
  "paper_url": "string",  // å¯ä»¥ä¸ºç©ºå­—ç¬¦ä¸²""
  "reproduction_script_url": "string",  // å¯ä»¥ä¸ºç©ºå­—ç¬¦ä¸²""
  "evaluation_metrics": ["string"]  // ä¸metricsç›¸åŒ
}}

=== ç¬¬8éƒ¨åˆ†ï¼šç‰¹æ®Šæƒ…å†µå¤„ç† ===

ã€æƒ…å†µ1ï¼šæ‘˜è¦å­—æ®µè¢«æ±¡æŸ“ã€‘
å¦‚æœabstractå­—æ®µåŒ…å«HTMLæ ‡ç­¾ã€Markdownè¯­æ³•ã€å›¾ç‰‡é“¾æ¥ï¼ˆå¦‚`<!-- <p align="center"> <img alt=...`ï¼‰ï¼Œè¯´æ˜è¿™æ˜¯GitHub READMEåŸå§‹å†…å®¹æœªæ¸…ç†ã€‚ä½ éœ€è¦ï¼š
- å°è¯•ä»æ±¡æŸ“å†…å®¹ä¸­æå–æœ‰ä»·å€¼çš„æ–‡æœ¬ä¿¡æ¯
- åœ¨æ¨ç†ä¸­æ˜ç¡®è¯´æ˜"æ‘˜è¦å­—æ®µè¢«æ±¡æŸ“ï¼Œä»READMEä¸­æå–åˆ°çš„æœ‰æ•ˆä¿¡æ¯æœ‰é™"
- é€‚å½“é™ä½reproducibility_scoreï¼ˆå› ä¸ºæ–‡æ¡£è´¨é‡å·®ï¼‰

ã€æƒ…å†µ2ï¼šç¼ºå°‘GitHub starsã€‘
å¦‚æœgithub_starsä¸ºnullæˆ–0ï¼Œä½†æœ‰GitHub URLï¼š
- è¯´æ˜"å€™é€‰æä¾›äº†GitHubé“¾æ¥ä½†æœªè·å–åˆ°starsæ•°æ®ï¼Œå¯èƒ½æ˜¯æ–°é¡¹ç›®æˆ–ç§æœ‰ä»“åº“"
- activity_scoreé€‚å½“ä¿å®ˆè¯„åˆ†ï¼ˆâ‰¤6åˆ†ï¼‰

ã€æƒ…å†µ3ï¼šéBenchmarkå€™é€‰ã€‘
å¦‚æœå‘ç°å€™é€‰ä¸æ˜¯Benchmarkè€Œæ˜¯å·¥å…·/æ¡†æ¶/åº“ï¼š
- åœ¨overall_reasoningä¸­æ˜ç¡®è¯´æ˜"è¯¥å€™é€‰ä¸æ˜¯Benchmarkï¼Œè€Œæ˜¯[å·¥å…·/æ¡†æ¶/åº“]"
- novelty_scoreå’Œrelevance_scoreç»™äºˆè¾ƒä½åˆ†æ•°ï¼ˆâ‰¤4åˆ†ï¼‰
- ä¸æ¨èçº³å…¥MGX Benchmarkæ± 

ã€æƒ…å†µ4ï¼šåç«¯Benchmarkè¯†åˆ«ã€‘
å¦‚æœå€™é€‰æ ‡é¢˜æˆ–æ‘˜è¦åŒ…å«ä»¥ä¸‹å…³é”®è¯ï¼Œåˆ¤æ–­ä¸ºåç«¯Benchmarkï¼š
- æ•°æ®åº“æ€§èƒ½: "database", "SQL", "NoSQL", "query optimization"
- Webæ¡†æ¶æ€§èƒ½: "web framework", "HTTP benchmark", "throughput", "latency"
- APIè®¾è®¡: "API benchmark", "RESTful", "GraphQL performance"
- åˆ†å¸ƒå¼ç³»ç»Ÿ: "distributed", "consensus", "replication"
åˆ™å¿…é¡»å¡«å†™backend_mgx_relevanceå’Œbackend_engineering_valueåŠå…¶æ¨ç†ã€‚

=== ç¬¬9éƒ¨åˆ†ï¼šè´¨é‡æ£€æŸ¥æ¸…å• ===

è¾“å‡ºJSONå‰ï¼Œè¯·è‡ªæ£€ï¼š
- [ ] æ‰€æœ‰scoreå­—æ®µåœ¨0-10èŒƒå›´å†…
- [ ] activity_reasoning â‰¥ 150å­—ç¬¦
- [ ] reproducibility_reasoning â‰¥ 150å­—ç¬¦
- [ ] license_reasoning â‰¥ 150å­—ç¬¦
- [ ] novelty_reasoning â‰¥ 150å­—ç¬¦
- [ ] relevance_reasoning â‰¥ 150å­—ç¬¦
- [ ] å¦‚æœæ˜¯åç«¯Benchmark: backend_mgx_reasoning â‰¥ 200å­—ç¬¦, backend_engineering_reasoning â‰¥ 200å­—ç¬¦
- [ ] overall_reasoning â‰¥ 200å­—ç¬¦
- [ ] task_domainä¸æ˜¯nullï¼Œä»é¢„å®šä¹‰åˆ—è¡¨ä¸­é€‰æ‹©
- [ ] institutionä¸æ˜¯nullï¼ˆå¯ä»¥æ˜¯"Unknown"ï¼‰
- [ ] task_typeä¸æ˜¯nullï¼ˆå¯ä»¥æ˜¯"Other"ï¼‰
- [ ] license_typeä¸æ˜¯nullï¼ˆå¯ä»¥æ˜¯"Unknown"ï¼‰
- [ ] dataset_size_descriptionä¸æ˜¯nullï¼ˆå¯ä»¥æ˜¯"Not specified"ï¼‰
- [ ] JSONä¸¥æ ¼ç¬¦åˆSchemaï¼Œæ²¡æœ‰å¤šä½™å­—æ®µ
- [ ] JSONå¯ä»¥è¢«æ ‡å‡†è§£æå™¨è§£æï¼ˆæ²¡æœ‰è¯­æ³•é”™è¯¯ï¼‰

ã€PDFæ·±åº¦å†…å®¹ (Phase PDF Enhancement)ã€‘
> Introductionéƒ¨åˆ†æ‘˜è¦ (2000å­—):
{introduction_summary}

> Method/Approachéƒ¨åˆ†æ‘˜è¦ (3000å­—):
{method_summary}

> Evaluation/Experimentséƒ¨åˆ†æ‘˜è¦ (3000å­—):
{evaluation_summary}

> Dataset/Dataéƒ¨åˆ†æ‘˜è¦ (2000å­—):
{dataset_summary}

> Baselines/Related Workéƒ¨åˆ†æ‘˜è¦ (2000å­—):
{baselines_summary}

> Conclusion/Discussionéƒ¨åˆ†æ‘˜è¦ (2000å­—):
{conclusion_summary}

ã€åŸå§‹æå–æ•°æ® (é‡‡é›†å™¨ç²—æå–)ã€‘
- åŸå§‹æŒ‡æ ‡: {raw_metrics}
- åŸå§‹Baseline: {raw_baselines}
- åŸå§‹ä½œè€…: {raw_authors}
- åŸå§‹æœºæ„: {raw_institutions}
- åŸå§‹æ•°æ®è§„æ¨¡: {raw_dataset_size}

ç°åœ¨ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°9ä¸ªéƒ¨åˆ†çš„è¦æ±‚ï¼Œè¾“å‡ºè§„èŒƒçš„JSONè¯„åˆ†ç»“æœã€‚**ä¸è¦åœ¨JSONå‰åæ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ï¼Œç›´æ¥è¾“å‡ºçº¯JSONã€‚**
"""


# ==================== Pydanticæ•°æ®æ¨¡å‹ ====================
class UnifiedBenchmarkExtraction(BaseModel):
    """å…¨LLMç»Ÿä¸€è¯„åˆ†è¾“å‡ºæ¨¡å‹ï¼ˆ26ä¸ªå­—æ®µï¼‰"""

    model_config = ConfigDict(extra="forbid")  # ç¦æ­¢é¢å¤–å­—æ®µ

    # 5ç»´è¯„åˆ†
    activity_score: float = Field(..., ge=0.0, le=10.0)
    reproducibility_score: float = Field(..., ge=0.0, le=10.0)
    license_score: float = Field(..., ge=0.0, le=10.0)
    novelty_score: float = Field(..., ge=0.0, le=10.0)
    relevance_score: float = Field(..., ge=0.0, le=10.0)

    # 5ç»´è¯¦ç»†æ¨ç†ï¼ˆæ¯ä¸ªâ‰¥150å­—ç¬¦ï¼‰
    activity_reasoning: str = Field(
        ..., min_length=constants.LLM_REASONING_MIN_CHARS
    )
    reproducibility_reasoning: str = Field(
        ..., min_length=constants.LLM_REASONING_MIN_CHARS
    )
    license_reasoning: str = Field(
        ..., min_length=constants.LLM_REASONING_MIN_CHARS
    )
    novelty_reasoning: str = Field(
        ..., min_length=constants.LLM_REASONING_MIN_CHARS
    )
    relevance_reasoning: str = Field(
        ..., min_length=constants.LLM_REASONING_MIN_CHARS
    )

    # åç«¯ä¸“é¡¹è¯„åˆ†ï¼ˆä»…åç«¯Benchmarkï¼‰
    backend_mgx_relevance: float = Field(default=0.0, ge=0.0, le=10.0)
    backend_mgx_reasoning: str = Field(default="")
    backend_engineering_value: float = Field(default=0.0, ge=0.0, le=10.0)
    backend_engineering_reasoning: str = Field(default="")

    # ç»¼åˆæ¨ç†
    overall_reasoning: str = Field(
        ..., min_length=constants.LLM_OVERALL_REASONING_MIN_CHARS
    )

    # ç»“æ„åŒ–å­—æ®µ
    task_domain: str  # å¿…å¡«ï¼Œä¸èƒ½æ˜¯None
    metrics: List[str] = Field(default_factory=list)
    baselines: List[str] = Field(default_factory=list)
    institution: str  # å¿…å¡«ï¼Œä¸èƒ½æ˜¯None
    authors: List[str] = Field(default_factory=list)
    dataset_size: Optional[int] = None
    dataset_size_description: str  # å¿…å¡«
    task_type: str  # å¿…å¡«
    license_type: str  # å¿…å¡«
    paper_url: str = ""
    reproduction_script_url: str = ""
    evaluation_metrics: List[str] = Field(default_factory=list)

    @field_validator("backend_mgx_reasoning", "backend_engineering_reasoning")
    @classmethod
    def validate_backend_reasoning(cls, v: str, info) -> str:
        """åç«¯æ¨ç†å­—æ®µéªŒè¯ï¼šå¦‚æœåç«¯è¯„åˆ†>0ï¼Œæ¨ç†å¿…é¡»â‰¥200å­—ç¬¦"""
        data = info.data
        required = constants.LLM_BACKEND_REASONING_MIN_CHARS
        needs_backend_reasoning = False
        if data.get("backend_mgx_relevance", 0) > 0:
            needs_backend_reasoning = True
        if info.field_name == "backend_engineering_reasoning":
            needs_backend_reasoning = needs_backend_reasoning or (
                data.get("backend_engineering_value", 0) > 0
            )
        if needs_backend_reasoning and len(v) < required:
            raise ValueError(
                f"åç«¯æ¨ç†å­—æ®µå¿…é¡»â‰¥{required}å­—ç¬¦ï¼Œå½“å‰{len(v)}å­—ç¬¦"
            )
        return v


# ==================== LLMè¯„åˆ†å¼•æ“ ====================
class LLMScorer:
    """å…¨LLMç»Ÿä¸€è¯„åˆ†å¼•æ“ï¼ˆå•æ¬¡è°ƒç”¨è¿”å›æ‰€æœ‰26ä¸ªå­—æ®µï¼‰"""

    def __init__(self) -> None:
        self.settings = get_settings()
        api_key = self.settings.openai.api_key
        base_url = self.settings.openai.base_url
        self.client: Optional[AsyncOpenAI] = None
        if api_key:
            self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.redis_client: Optional[AsyncRedis] = None

    async def __aenter__(self) -> "LLMScorer":
        try:
            self.redis_client = redis.from_url(
                self.settings.redis.url,
                encoding="utf-8",
                decode_responses=True,
            )
            ping_future = cast(Awaitable[bool], self.redis_client.ping())
            await ping_future
        except Exception as exc:  # noqa: BLE001
            logger.warning("Redisè¿æ¥å¤±è´¥,å°†ä¸ä½¿ç”¨ç¼“å­˜: %s", exc)
            self.redis_client = None
        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:
        if self.redis_client:
            await self.redis_client.aclose()
            self.redis_client = None

    def _cache_key(self, candidate: RawCandidate) -> str:
        """ç”ŸæˆRedisç¼“å­˜é”®ï¼ˆåŸºäºæ ‡é¢˜+URLçš„MD5ï¼‰"""
        key_str = f"v2:{candidate.title}:{candidate.url}"  # v2è¡¨ç¤ºæ–°ç‰ˆprompt
        digest = hashlib.md5(
            key_str.encode(), usedforsecurity=False
        ).hexdigest()
        return f"{constants.REDIS_KEY_PREFIX}unified_score:{digest}"

    async def _get_cached_score(
        self, candidate: RawCandidate
    ) -> Optional[UnifiedBenchmarkExtraction]:
        """ä»Redisè¯»å–ç¼“å­˜è¯„åˆ†"""
        if not self.redis_client:
            return None
        try:
            cached = await self.redis_client.get(self._cache_key(candidate))
            if cached:
                logger.debug("è¯„åˆ†ç¼“å­˜å‘½ä¸­: %s", candidate.title[:50])
                return UnifiedBenchmarkExtraction.parse_raw(cached)
        except Exception as exc:  # noqa: BLE001
            logger.warning("è¯»å–Redisç¼“å­˜å¤±è´¥: %s", exc)
        return None

    async def _set_cached_score(
        self, candidate: RawCandidate, extraction: UnifiedBenchmarkExtraction
    ) -> None:
        """å°†è¯„åˆ†ç»“æœå†™å…¥Redisç¼“å­˜"""
        if not self.redis_client:
            return
        try:
            await self.redis_client.setex(
                self._cache_key(candidate),
                constants.REDIS_TTL_DAYS * 86400,
                extraction.json(),
            )
        except Exception as exc:  # noqa: BLE001
            logger.warning("å†™å…¥Redisç¼“å­˜å¤±è´¥: %s", exc)

    def _build_prompt(self, candidate: RawCandidate) -> str:
        """æ„å»º4000+ tokençš„è¶…è¯¦ç»†è¯„åˆ†prompt"""
        abstract = clean_summary_text(candidate.abstract or "æ— ") or "æ— "
        if len(abstract) > 2000:
            abstract = abstract[:2000] + "..."

        # æå–PDFå¢å¼ºå†…å®¹
        raw_metadata = candidate.raw_metadata or {}
        introduction_summary = raw_metadata.get("introduction_summary") or "æœªæä¾›ï¼ˆè®ºæ–‡æ— Introductionç« èŠ‚æˆ–PDFè§£æå¤±è´¥ï¼‰"
        method_summary = raw_metadata.get("method_summary") or "æœªæä¾›ï¼ˆè®ºæ–‡æ— Methodç« èŠ‚æˆ–PDFè§£æå¤±è´¥ï¼‰"
        evaluation_summary = raw_metadata.get("evaluation_summary") or "æœªæä¾›ï¼ˆè®ºæ–‡æ— Evaluationç« èŠ‚æˆ–PDFè§£æå¤±è´¥ï¼‰"
        dataset_summary = raw_metadata.get("dataset_summary") or "æœªæä¾›ï¼ˆè®ºæ–‡æ— Datasetç« èŠ‚æˆ–PDFè§£æå¤±è´¥ï¼‰"
        baselines_summary = raw_metadata.get("baselines_summary") or "æœªæä¾›ï¼ˆè®ºæ–‡æ— Baselinesç« èŠ‚æˆ–PDFè§£æå¤±è´¥ï¼‰"
        conclusion_summary = raw_metadata.get("conclusion_summary") or "æœªæä¾›ï¼ˆè®ºæ–‡æ— Conclusionç« èŠ‚æˆ–PDFè§£æå¤±è´¥ï¼‰"

        # åŸå§‹æå–æ•°æ®
        raw_metrics = ", ".join(candidate.raw_metrics or []) if candidate.raw_metrics else "æœªæå–"
        raw_baselines = ", ".join(candidate.raw_baselines or []) if candidate.raw_baselines else "æœªæå–"
        raw_authors = candidate.raw_authors or (", ".join(candidate.authors or []) if candidate.authors else "æœªæå–")
        raw_institutions = candidate.raw_institutions or "æœªæå–"
        raw_dataset = candidate.raw_dataset_size or "æœªæå–"

        return UNIFIED_SCORING_PROMPT_TEMPLATE.format(
            task_domain_options=", ".join(constants.TASK_DOMAIN_OPTIONS),
            max_metrics=constants.MAX_EXTRACTED_METRICS,
            title=candidate.title,
            source=candidate.source,
            url=candidate.url,
            abstract=abstract,
            github_stars=candidate.github_stars or "æœªæä¾›",
            publish_date=candidate.publish_date.strftime("%Y-%m-%d") if candidate.publish_date else "æœªçŸ¥",
            github_url=candidate.github_url or "æœªæä¾›",
            dataset_url=candidate.dataset_url or "æœªæä¾›",
            paper_url=candidate.paper_url or "æœªæä¾›",
            license_type=candidate.license_type or "æœªçŸ¥",
            task_type=candidate.task_type or "æœªè¯†åˆ«",
            introduction_summary=introduction_summary,
            method_summary=method_summary,
            evaluation_summary=evaluation_summary,
            dataset_summary=dataset_summary,
            baselines_summary=baselines_summary,
            conclusion_summary=conclusion_summary,
            raw_metrics=raw_metrics,
            raw_baselines=raw_baselines,
            raw_authors=raw_authors,
            raw_institutions=raw_institutions,
            raw_dataset_size=raw_dataset,
        )

    @retry(
        stop=stop_after_attempt(constants.LLM_MAX_RETRIES),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    async def _call_llm(
        self, candidate: RawCandidate
    ) -> UnifiedBenchmarkExtraction:
        """è°ƒç”¨LLMè·å–è¯„åˆ†ï¼ˆå•æ¬¡è¿”å›æ‰€æœ‰26ä¸ªå­—æ®µï¼‰"""
        if not self.client:
            raise RuntimeError("æœªé…ç½®OpenAIæ¥å£,æ— æ³•è°ƒç”¨LLM")

        prompt = self._build_prompt(candidate)
        logger.debug("LLMè¯„åˆ†prompté•¿åº¦: %d å­—ç¬¦", len(prompt))

        messages: List[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯MGX BenchScopeçš„Benchmarkè¯„ä¼°ä¸“å®¶ï¼Œå°†è¾“å‡ºå¯ç›´æ¥å…¥åº“çš„JSONè¯„åˆ†ç»“æœã€‚\n\n"
                    "ã€å…³é”®ç¡¬æ€§è¦æ±‚â€”â€”è¿åä»»æ„ä¸€æ¡å°†è§†ä¸ºå¤±è´¥ã€‘\n"
                    "1. activity/reproducibility/license/novelty/relevance_reasoning å„â‰¥150å­—ç¬¦ï¼ˆå»ºè®®â‰¥180å­—ç¬¦ï¼‰\n"
                    "2. è‹¥ backend_mgx_relevance æˆ– backend_engineering_value > 0ï¼Œåˆ™å¯¹åº”çš„ backend_*_reasoning å„â‰¥200å­—ç¬¦ï¼›å¦åˆ™å¯ç•™ç©ºå­—ç¬¦ä¸²\n"
                    "3. overall_reasoning â‰¥ 200å­—ç¬¦ï¼Œéœ€è¦æ€»ç»“æ¨èæ„è§ã€ä¼˜åŠ¿ä¸é£é™©\n"
                    f"4. æ€»æ¨ç†å­—æ•°â‰¥{constants.LLM_TOTAL_REASONING_MIN_CHARS}å­—ç¬¦ï¼ˆå³ä¾¿æ— åç«¯å­—æ®µï¼Œä¹Ÿéœ€é€šè¿‡å±•å¼€ç»†èŠ‚æ»¡è¶³è¦æ±‚ï¼‰\n\n"
                    "ã€å¦‚ä½•ä¿è¯å­—ç¬¦è¦æ±‚ã€‘\n"
                    "- æä¾›å…·ä½“æ•°æ®ï¼ˆGitHub starsã€æäº¤æ—¶é—´ã€PR/Issueæ•°é‡ã€ç®—åŠ›éœ€æ±‚ç­‰ï¼‰å¹¶å±•å¼€è®ºè¿°\n"
                    "- æ¯ä¸ªæ¨ç†æ®µè½ç»“æ„ä¸ºâ€œè¯æ®â†’åˆ†æâ†’ç»“è®ºâ€ï¼Œè‡³å°‘2-3å¥è¯\n"
                    "- å¦‚æœä¿¡æ¯ä¸è¶³ä¹Ÿè¦å†™æ˜æ¨æ–­ä¾æ®ä¸æ½œåœ¨é£é™©ï¼Œä¸å¾—ä»¥ä¸€å¥è¯å¸¦è¿‡\n"
                    "- è¾“å‡ºå‰è‡ªè¡Œæ£€æŸ¥å­—ç¬¦æ•°ï¼›è‹¥ä¸æ»¡è¶³è¦æ±‚ï¼Œç»§ç»­è¡¥å……ç»†èŠ‚å†è¾“å‡º\n\n"
                    "ã€è¾“å‡ºé™åˆ¶ã€‘ä¸¥æ ¼éµå¾ªç»™å®šJSON Schemaï¼Œä¸å…è®¸æ–°å¢/ç¼ºå¤±å­—æ®µï¼Œä¸å¾—è¿”å› nullï¼ˆé™¤æ˜ç¡®å…è®¸ï¼‰ã€‚"
                ),
            },
            {"role": "user", "content": prompt},
        ]

        repair_attempt = 0
        while True:
            response = await asyncio.wait_for(
                self.client.chat.completions.create(
                    model=self.settings.openai.model or constants.LLM_MODEL,
                    messages=messages,
                    temperature=0.1,
                    max_tokens=4096,  # å¢åŠ max_tokensä»¥å®¹çº³è¯¦ç»†æ¨ç†
                ),
                timeout=constants.LLM_TIMEOUT_SECONDS * 2,  # å¢åŠ è¶…æ—¶æ—¶é—´
            )

            content = response.choices[0].message.content or ""
            logger.debug("LLMåŸå§‹å“åº”é•¿åº¦: %d å­—ç¬¦", len(content))
            payload = self._load_payload(content)
            try:
                extraction = UnifiedBenchmarkExtraction.parse_obj(payload)
            except ValidationError as exc:  # noqa: PERF203
                violations = self._extract_length_violations(exc, payload)
                if (
                    violations
                    and repair_attempt < constants.LLM_SELF_HEAL_MAX_ATTEMPTS
                ):
                    repair_attempt += 1
                    fix_prompt = self._build_length_fix_prompt(violations)
                    messages.append({"role": "assistant", "content": content})
                    messages.append({"role": "user", "content": fix_prompt})
                    logger.debug(
                        "LLMæ¨ç†é•¿åº¦ä¸è¶³ï¼Œè§¦å‘ç¬¬%dæ¬¡çº å: %s",
                        repair_attempt,
                        candidate.title[:50],
                    )
                    continue
                # å…œåº•ï¼šçº åç”¨å°½åï¼Œå¯¹é•¿åº¦ä¸è¶³å­—æ®µåšè‡ªåŠ¨å¡«å……å†å°è¯•ä¸€æ¬¡
                autofixed_payload = self._autofix_payload_lengths(payload)
                try:
                    extraction = UnifiedBenchmarkExtraction.parse_obj(autofixed_payload)
                    logger.warning(
                        "LLMå“åº”é•¿åº¦ä¸è¶³å·²é€šè¿‡è‡ªåŠ¨å…œåº•ä¿®å¤: %s",
                        candidate.title[:50],
                    )
                except ValidationError:
                    logger.error("LLMå“åº”å­—æ®µæ ¡éªŒå¤±è´¥: %s", exc)
                    logger.error(
                        "è§£æçš„payload: %s",
                        json.dumps(payload, indent=2, ensure_ascii=False)[:1000],
                    )
                    raise

            # æ£€æŸ¥æ€»æ¨ç†é•¿åº¦ï¼Œä¸è¶³åˆ™å°è¯•è‡ªæ„ˆçº å
            total_reasoning_length = (
                len(extraction.activity_reasoning)
                + len(extraction.reproducibility_reasoning)
                + len(extraction.license_reasoning)
                + len(extraction.novelty_reasoning)
                + len(extraction.relevance_reasoning)
                + len(extraction.backend_mgx_reasoning)
                + len(extraction.backend_engineering_reasoning)
                + len(extraction.overall_reasoning)
            )
            min_total_chars = constants.LLM_TOTAL_REASONING_MIN_CHARS
            if (
                total_reasoning_length < min_total_chars
                and repair_attempt < constants.LLM_SELF_HEAL_MAX_ATTEMPTS
            ):
                repair_attempt += 1
                shortage = min_total_chars - total_reasoning_length
                fix_prompt = (
                    "ä¸Šä¸€æ¬¡JSONè¾“å‡ºçš„æ¨ç†æ€»å­—æ•°ä¸è¶³ï¼š"
                    f"å½“å‰{total_reasoning_length}å­—ç¬¦ï¼Œè¦æ±‚â‰¥{min_total_chars}å­—ç¬¦ï¼ˆå·®{shortage}å­—ç¬¦ï¼‰ã€‚\n\n"
                    "è¯·ä¿ç•™æ‰€æœ‰å­—æ®µå¹¶é‡æ–°è¾“å‡ºå®Œæ•´JSONï¼Œé€šè¿‡ä»¥ä¸‹æ–¹å¼æ‰©å†™æ¨ç†æ®µè½ï¼š\n"
                    "1. è¡¥å……å…·ä½“æ•°æ®ï¼ˆGitHub starsã€PRæ•°é‡ã€æäº¤æ—¶é—´ã€ç®—åŠ›éœ€æ±‚ç­‰ï¼‰\n"
                    "2. å±•å¼€è®ºè¯ç»“æ„ï¼š\"è¯æ®â†’åˆ†æâ†’ç»“è®º\"ï¼Œæ¯ä¸ªæ¨ç†æ®µè½è‡³å°‘2-3å¥è¯\n"
                    "3. æ˜ç¡®æŒ‡å‡ºæ½œåœ¨é£é™©å’Œä¸è¶³ï¼Œä¸è¦åªå†™ä¼˜ç‚¹\n"
                    "4. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œå†™æ˜æ¨æ–­ä¾æ®ä¸å±€é™æ€§\n\n"
                    "åªè¾“å‡ºç¬¦åˆSchemaçš„çº¯JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šæˆ–çœç•¥å­—æ®µã€‚"
                )

                messages.append({"role": "assistant", "content": content})
                messages.append({"role": "user", "content": fix_prompt})
                logger.warning(
                    "æ¨ç†æ€»å­—æ•°ä¸è¶³ï¼ˆ%d < %dï¼‰ï¼Œè§¦å‘ç¬¬%dæ¬¡çº å: %s",
                    total_reasoning_length,
                    min_total_chars,
                    repair_attempt,
                    candidate.title[:50],
                )
                continue

            if total_reasoning_length < min_total_chars:
                logger.warning(
                    "æ¨ç†æ€»å­—æ•°ä¸è¶³: %d < %dï¼ˆå·²è¾¾æœ€å¤§é‡è¯•%dæ¬¡ï¼‰ï¼Œå€™é€‰ï¼š%s",
                    total_reasoning_length,
                    min_total_chars,
                    constants.LLM_SELF_HEAL_MAX_ATTEMPTS,
                    candidate.title[:50],
                )

            return extraction



    def _load_payload(self, content: str) -> dict[str, Any]:
        """è§£æLLMå“åº”æ–‡æœ¬ä¸ºJSONå¯¹è±¡"""
        json_str = self._strip_code_fence(content)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as exc:  # noqa: BLE001
            logger.error("LLMå“åº”è§£æå¤±è´¥(JSON): %s", exc)
            logger.error("åŸå§‹å“åº”: %s", content[:1000])
            raise

    @staticmethod
    def _strip_code_fence(content: str) -> str:
        """å»é™¤Markdownä»£ç å—æ ‡è®°"""
        text = content.strip()
        if text.startswith("```") and text.endswith("```"):
            lines = text.split("\n")
            lines = lines[1:-1]
            return "\n".join(lines).strip()
        if text.startswith("```"):
            return text.split("\n", 1)[-1].rsplit("```", 1)[0].strip()
        return text

    def _extract_length_violations(
        self, error: ValidationError, payload: dict[str, Any]
    ) -> Dict[str, Tuple[int, int]]:
        """ä»Pydanticé”™è¯¯ä¸­æå–å¯è‡ªåŠ¨ä¿®å¤çš„å­—ç¬¦é•¿åº¦é—®é¢˜"""
        violations: Dict[str, Tuple[int, int]] = {}
        for err in error.errors():
            loc = err.get("loc") or ()
            field = loc[0] if loc else None
            if not isinstance(field, str):
                continue

            min_length: Optional[int] = None
            err_type = err.get("type")
            if err_type == "string_too_short":
                min_length = err.get("ctx", {}).get("min_length")
            elif err_type == "value_error" and "åç«¯æ¨ç†å­—æ®µ" in err.get("msg", ""):
                min_length = constants.LLM_BACKEND_REASONING_MIN_CHARS
            else:
                continue

            if not isinstance(min_length, int):
                continue

            current_value = payload.get(field, "") or ""
            current_length = len(str(current_value))
            violations[field] = (min_length, current_length)

        return violations

    def _build_length_fix_prompt(
        self, violations: Dict[str, Tuple[int, int]]
    ) -> str:
        """æ„é€ æç¤ºè¯­ï¼Œè®©LLMæ‰©å†™å­—ç¬¦ä¸è¶³çš„æ¨ç†å­—æ®µ"""
        ordered_fields: List[str] = []
        for field in REASONING_FIELD_ORDER:
            if field in violations:
                ordered_fields.append(field)
        for field in violations:
            if field not in ordered_fields:
                ordered_fields.append(field)

        tips = [
            "ä¸Šä¸€æ¬¡çš„JSONè¾“å‡ºæœªé€šè¿‡æ ¡éªŒï¼šä»¥ä¸‹æ¨ç†å­—æ®µå­—ç¬¦æ•°ä¸è¶³ã€‚",
            "è¯·ä¿ç•™æ‰€æœ‰å­—æ®µå¹¶é‡æ–°è¾“å‡ºå®Œæ•´JSONï¼Œé€šè¿‡è¡¥å……è¯æ®ã€æ•°æ®æ¥æºã€MGXåœºæ™¯å½±å“ã€æ½œåœ¨é£é™©ç­‰æ–¹å¼æ‰©å†™å¯¹åº”æ¨ç†æ®µè½ã€‚",
        ]
        for field in ordered_fields:
            required, current = violations[field]
            label = REASONING_FIELD_LABELS.get(field, field)
            tips.append(
                f"- {label}: å½“å‰{current}å­—ç¬¦ï¼Œè‡³å°‘{required}å­—ç¬¦ã€‚"
            )
        tips.append("åªè¾“å‡ºç¬¦åˆSchemaçš„çº¯JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šæˆ–çœç•¥å­—æ®µã€‚")
        return "\n".join(tips)

    @staticmethod
    def _autofix_payload_lengths(payload: dict) -> dict:
        """åœ¨å¤šæ¬¡çº åä»å¤±è´¥æ—¶ï¼Œå¯¹å…³é”®å­—æ®µåšå…œåº•æ‰©å†™ï¼Œé¿å…æ•´æ‰¹å¤±è´¥ã€‚

        å½“å‰ä»…å…œåº• overall_reasoningï¼šè‹¥é•¿åº¦ä¸è¶³ï¼Œæ‹¼æ¥å„ç»´åº¦æ¨ç†æ‘˜è¦ç”Ÿæˆåˆè§„æ–‡æœ¬ã€‚
        """

        overall = (payload.get("overall_reasoning") or "").strip()
        min_len = constants.LLM_OVERALL_REASONING_MIN_CHARS
        if len(overall) >= min_len:
            return payload

        parts: list[str] = []
        for key in [
            "activity_reasoning",
            "reproducibility_reasoning",
            "license_reasoning",
            "novelty_reasoning",
            "relevance_reasoning",
        ]:
            text = (payload.get(key) or "").strip()
            if text:
                parts.append(text)

        if parts:
            overall_fixed = (
                f"{overall} {' '.join(parts)} ç»¼ä¸Šï¼šç»“åˆæ´»è·ƒåº¦ã€å¯å¤ç°æ€§ã€è®¸å¯ä¸ç›¸å…³æ€§è¯„ä¼°ï¼Œ"
                "éœ€è¡¥å……æ–‡æ¡£ä¸åˆè§„ç¡®è®¤åå†å†³å®šæ˜¯å¦çº³å…¥ï¼Œå½“å‰å»ºè®®æš‚ç¼“é‡‡çº³ä»¥é™ä½å·¥ç¨‹ä¸æ³•å¾‹é£é™©ã€‚"
            ).strip()
        else:
            overall_fixed = (
                "ç»¼åˆä¿¡æ¯ä¸è¶³ï¼Œéœ€è¡¥å……ä»£ç ã€å¤ç°è„šæœ¬ã€è®¸å¯è¯ä¸ä»»åŠ¡æè¿°åå†è¯„ä¼°ï¼›"
                "åœ¨æ˜ç¡®æ´»è·ƒåº¦ã€å¯å¤ç°æ€§å’Œé€‚é…åº¦ä¹‹å‰ï¼Œä¸å»ºè®®MGXçº³å…¥ï¼Œä»¥é¿å…å·¥ç¨‹ä¸åˆè§„é£é™©ã€‚"
            )

        # ç¡®ä¿é•¿åº¦è¾¾åˆ°ä¸‹é™
        if len(overall_fixed) < min_len:
            overall_fixed = overall_fixed + "ã€‚" * (min_len - len(overall_fixed))

        payload["overall_reasoning"] = overall_fixed
        return payload

    async def score(self, candidate: RawCandidate) -> ScoredCandidate:
        """è¯„åˆ†å•ä¸ªå€™é€‰é¡¹"""
        # å°è¯•è¯»å–ç¼“å­˜
        extraction = await self._get_cached_score(candidate)
        if not extraction:
            if not self.client:
                logger.error("OpenAIæœªé…ç½®ä¸”æ— ç¼“å­˜,æ— æ³•è¯„åˆ†: %s", candidate.title[:50])
                raise RuntimeError("æœªé…ç½®OpenAIä¸”æ— ç¼“å­˜,æ— æ³•è¯„åˆ†")
            try:
                extraction = await self._call_llm(candidate)
                await self._set_cached_score(candidate, extraction)
            except Exception as exc:
                logger.error("LLMè¯„åˆ†å¤±è´¥: %s, å€™é€‰: %s", exc, candidate.title[:50])
                raise

        return self._to_scored_candidate(candidate, extraction)

    def _to_scored_candidate(
        self,
        candidate: RawCandidate,
        extraction: UnifiedBenchmarkExtraction,
    ) -> ScoredCandidate:
        """å°†è¯„åˆ†ç»“æœè½¬æ¢ä¸ºScoredCandidate"""
        # åˆå¹¶ä½œè€…ä¿¡æ¯
        authors = extraction.authors or candidate.authors
        # åˆå¹¶æŒ‡æ ‡ä¿¡æ¯
        metrics = extraction.metrics or candidate.evaluation_metrics
        # æœºæ„ä¿¡æ¯
        institution = extraction.institution if extraction.institution != "Unknown" else candidate.raw_institutions
        # æ•°æ®é›†è§„æ¨¡æè¿°
        dataset_size_desc = (
            extraction.dataset_size_description
            if extraction.dataset_size_description != "Not specified"
            else candidate.raw_dataset_size
        )

        # æ„å»ºscore_reasoningï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
        score_reasoning = (
            f"ã€ç»¼åˆæ¨ç†ã€‘{extraction.overall_reasoning}\n\n"
            f"ã€æ´»è·ƒåº¦ã€‘{extraction.activity_reasoning}\n\n"
            f"ã€å¯å¤ç°æ€§ã€‘{extraction.reproducibility_reasoning}\n\n"
            f"ã€è®¸å¯åˆè§„ã€‘{extraction.license_reasoning}\n\n"
            f"ã€æ–°é¢–æ€§ã€‘{extraction.novelty_reasoning}\n\n"
            f"ã€MGXé€‚é…åº¦ã€‘{extraction.relevance_reasoning}"
        )
        if extraction.backend_mgx_reasoning:
            score_reasoning += (
                f"\n\nã€åç«¯MGXç›¸å…³æ€§ã€‘{extraction.backend_mgx_reasoning}\n\n"
                f"ã€åç«¯å·¥ç¨‹ä»·å€¼ã€‘{extraction.backend_engineering_reasoning}"
            )

        return ScoredCandidate(
            # RawCandidateå­—æ®µ
            title=candidate.title,
            url=candidate.url,
            source=candidate.source,
            abstract=candidate.abstract,
            authors=authors,
            publish_date=candidate.publish_date,
            github_stars=candidate.github_stars,
            github_url=candidate.github_url,
            dataset_url=candidate.dataset_url,
            hero_image_url=candidate.hero_image_url,
            hero_image_key=candidate.hero_image_key,
            raw_metadata=candidate.raw_metadata,
            raw_metrics=candidate.raw_metrics,
            raw_baselines=candidate.raw_baselines,
            raw_authors=candidate.raw_authors,
            raw_institutions=candidate.raw_institutions,
            raw_dataset_size=candidate.raw_dataset_size,
            # Phase 6å­—æ®µ
            paper_url=extraction.paper_url or candidate.paper_url,
            task_type=extraction.task_type,
            license_type=extraction.license_type,
            evaluation_metrics=extraction.evaluation_metrics or candidate.evaluation_metrics,
            reproduction_script_url=extraction.reproduction_script_url or candidate.reproduction_script_url,
            # 5ç»´è¯„åˆ†
            activity_score=extraction.activity_score,
            reproducibility_score=extraction.reproducibility_score,
            license_score=extraction.license_score,
            novelty_score=extraction.novelty_score,
            relevance_score=extraction.relevance_score,
            # å…¼å®¹æ—§ç‰ˆscore_reasoning
            score_reasoning=score_reasoning,
            # æ–°å¢è¯¦ç»†æ¨ç†å­—æ®µ
            activity_reasoning=extraction.activity_reasoning,
            reproducibility_reasoning=extraction.reproducibility_reasoning,
            license_reasoning=extraction.license_reasoning,
            novelty_reasoning=extraction.novelty_reasoning,
            relevance_reasoning=extraction.relevance_reasoning,
            # åç«¯ä¸“é¡¹è¯„åˆ†
            backend_mgx_relevance=extraction.backend_mgx_relevance,
            backend_mgx_reasoning=extraction.backend_mgx_reasoning,
            backend_engineering_value=extraction.backend_engineering_value,
            backend_engineering_reasoning=extraction.backend_engineering_reasoning,
            # ç»¼åˆæ¨ç†
            overall_reasoning=extraction.overall_reasoning,
            # Phase 8å­—æ®µ
            task_domain=extraction.task_domain,
            metrics=metrics,
            baselines=extraction.baselines,
            institution=institution,
            dataset_size=extraction.dataset_size,
            dataset_size_description=dataset_size_desc,
        )

    async def score_batch(
        self, candidates: List[RawCandidate]
    ) -> List[ScoredCandidate]:
        """æ‰¹é‡è¯„åˆ†ï¼ˆå¹¶å‘æ§åˆ¶ï¼‰"""
        if not candidates:
            return []

        semaphore = asyncio.Semaphore(constants.SCORE_CONCURRENCY)

        async def score_with_semaphore(candidate: RawCandidate) -> ScoredCandidate:
            async with semaphore:
                return await self.score(candidate)

        tasks = [score_with_semaphore(candidate) for candidate in candidates]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        scored_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(
                    "è¯„åˆ†å¤±è´¥: %s, å€™é€‰: %s",
                    result,
                    candidates[i].title[:50],
                )
            else:
                scored_results.append(result)

        logger.info(
            "æ‰¹é‡è¯„åˆ†å®Œæˆ: æˆåŠŸ%dæ¡/å…±%dæ¡ (å¹¶å‘ä¸Šé™=%d)",
            len(scored_results),
            len(candidates),
            constants.SCORE_CONCURRENCY,
        )
        return scored_results
